#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹æ¨¡å‹å±‚ - è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ– + å¤šCPUå¹¶è¡Œè®­ç»ƒ
ç›´æ¥ä½¿ç”¨ç°æœ‰çš„feature_engineering.pyè¾“å‡ºï¼Œå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦

ä¼˜åŒ–ç‰¹æ€§ï¼š
1. è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ï¼ˆä½¿ç”¨optunaï¼‰
2. å¤šCPUå¹¶è¡Œè®­ç»ƒ
3. ç›´æ¥ä½¿ç”¨ç°æœ‰ç‰¹å¾å·¥ç¨‹ç»“æœ
4. å†…å­˜ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶

ä½œè€…: AIåŠ©æ‰‹  
æ—¥æœŸ: 2024å¹´
ç‰ˆæœ¬: v2.0 - è¶…çº§ä¼˜åŒ–ç‰ˆ
"""

import pandas as pd
import numpy as np
import warnings
import time
import json
import logging
import os
import gc
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from multiprocessing import cpu_count
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import joblib
from functools import partial

# ç§‘å­¦è®¡ç®—å’Œæœºå™¨å­¦ä¹ 
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import silhouette_score, roc_auc_score
from sklearn.model_selection import cross_val_score
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner

# å¼‚å¸¸æ£€æµ‹ç®—æ³•
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.cluster import DBSCAN, KMeans
from sklearn.mixture import GaussianMixture
from sklearn.covariance import EllipticEnvelope
from sklearn.svm import OneClassSVM
from pyod.models.feature_bagging import FeatureBagging
from pyod.models.hbos import HBOS
from pyod.models.knn import KNN as PYOD_KNN
from pyod.models.lof import LOF as PYOD_LOF

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/anomaly_detection/logs/optimized_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizedAnomalyDetection:
    """ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹æ¨¡å‹ç±»"""
    
    def __init__(self, 
                 contamination: float = 0.05, 
                 random_state: int = 42,
                 n_jobs: int = -1,
                 optimization_trials: int = 20):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹æ¨¡å‹
        
        å‚æ•°:
            contamination: å¼‚å¸¸æ¯”ä¾‹ä¼°è®¡
            random_state: éšæœºç§å­
            n_jobs: å¹¶è¡Œå·¥ä½œæ•°é‡ (-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPU)
            optimization_trials: è´å¶æ–¯ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
        """
        self.contamination = contamination
        self.random_state = random_state
        self.n_jobs = n_jobs if n_jobs != -1 else cpu_count()
        self.optimization_trials = optimization_trials
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path('outputs/anomaly_detection')
        self.model_dir = self.output_dir / 'models'
        self.cache_dir = self.output_dir / 'cache'
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®å’Œæ¨¡å‹å­˜å‚¨
        self.data = None
        self.feature_names = []
        self.loop_ids = []
        self.trained_models = {}
        self.best_params = {}
        
        logger.info(f"åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹ - CPUæ ¸å¿ƒæ•°: {self.n_jobs}, ä¼˜åŒ–è¯•éªŒæ¬¡æ•°: {optimization_trials}")
    
    def load_engineered_features(self) -> bool:
        """åŠ è½½å·²ç»å·¥ç¨‹åŒ–çš„ç‰¹å¾æ•°æ®"""
        feature_file = "outputs/anomaly_detection/features/engineered_features.csv"
        
        if not Path(feature_file).exists():
            logger.error(f"æ‰¾ä¸åˆ°ç‰¹å¾æ–‡ä»¶: {feature_file}")
            logger.info("è¯·å…ˆè¿è¡Œ feature_engineering.py ç”Ÿæˆç‰¹å¾æ•°æ®")
            return False
        
        try:
            logger.info(f"åŠ è½½å·¥ç¨‹åŒ–ç‰¹å¾: {feature_file}")
            self.data = pd.read_csv(feature_file)
            
            # åˆ†ç¦»loop_idå’Œç‰¹å¾
            if 'loop_id' in self.data.columns:
                self.loop_ids = self.data['loop_id'].values
                features = self.data.drop('loop_id', axis=1)
            else:
                self.loop_ids = np.arange(len(self.data))
                features = self.data
            
            self.feature_names = features.columns.tolist()
            self.data = features
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.data)} ä¸ªç¯è·¯, {len(self.feature_names)} ä¸ªç‰¹å¾")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½ç‰¹å¾æ•°æ®å¤±è´¥: {e}")
            return False
    
    def preprocess_data(self) -> Dict[str, np.ndarray]:
        """å¿«é€Ÿæ•°æ®é¢„å¤„ç†"""
        logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # å¤„ç†ç¼ºå¤±å€¼å’Œæ— ç©·å€¼
        data_clean = self.data.fillna(self.data.median())
        data_clean = data_clean.replace([np.inf, -np.inf], np.nan)
        data_clean = data_clean.fillna(data_clean.median())
        
        processed_data = {}
        
        # å¤šç§æ ‡å‡†åŒ–æ–¹æ³•
        scalers = {
            'standard': StandardScaler(),
            'robust': RobustScaler(),
            'minmax': MinMaxScaler()
        }
        
        for name, scaler in scalers.items():
            processed_data[name] = scaler.fit_transform(data_clean)
            
        processed_data['raw'] = data_clean.values
        
        logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return processed_data
    
    def create_isolation_forest_objective(self, data: np.ndarray):
        """åˆ›å»ºIsolation Forestçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),
                'contamination': self.contamination,
                'random_state': self.random_state,
                'n_jobs': self.n_jobs
            }
            
            try:
                model = IsolationForest(**params)
                model.fit(data)
                scores = -model.decision_function(data)
                
                # ä½¿ç”¨å¼‚å¸¸åˆ†æ•°çš„åˆ†ç¦»åº¦ä½œä¸ºä¼˜åŒ–ç›®æ ‡
                score_std = np.std(scores)
                score_range = np.max(scores) - np.min(scores)
                objective_value = score_std * score_range  # åˆ†ç¦»åº¦è¶Šå¤§è¶Šå¥½
                
                return objective_value
            except:
                return 0.0
        
        return objective
    
    def create_lof_objective(self, data: np.ndarray):
        """åˆ›å»ºLOFçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'n_neighbors': trial.suggest_int('n_neighbors', 5, 50),
                'contamination': self.contamination,
                'n_jobs': self.n_jobs
            }
            
            try:
                model = LocalOutlierFactor(**params)
                labels = model.fit_predict(data)
                
                if len(np.unique(labels)) > 1:
                    scores = -model.negative_outlier_factor_
                    return np.std(scores) / (np.mean(scores) + 1e-6)
                else:
                    return 0.0
            except:
                return 0.0
        
        return objective
    
    def create_gmm_objective(self, data: np.ndarray):
        """åˆ›å»ºGMMçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'n_components': trial.suggest_int('n_components', 2, 10),
                'covariance_type': trial.suggest_categorical('covariance_type', ['full', 'diag', 'tied']),
                'random_state': self.random_state
            }
            
            try:
                model = GaussianMixture(**params)
                model.fit(data)
                score = model.score(data)  # å¯¹æ•°ä¼¼ç„¶
                return score
            except:
                return -np.inf
        
        return objective
    
    def create_kmeans_objective(self, data: np.ndarray):
        """åˆ›å»ºK-Meansçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'n_clusters': trial.suggest_int('n_clusters', 3, 20),
                'random_state': self.random_state,
                'n_init': 10
            }
            
            try:
                model = KMeans(**params)
                labels = model.fit_predict(data)
                
                if len(np.unique(labels)) > 1:
                    return silhouette_score(data, labels)
                else:
                    return -1.0
            except:
                return -1.0
        
        return objective
    
    def create_dbscan_objective(self, data: np.ndarray):
        """åˆ›å»ºDBSCANçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'eps': trial.suggest_float('eps', 0.05, 2.0),
                'min_samples': trial.suggest_int('min_samples', 3, 20)
            }
            
            try:
                model = DBSCAN(**params)
                labels = model.fit_predict(data)
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                if n_clusters > 0:
                    noise_ratio = np.sum(labels == -1) / len(labels)
                    if 0.01 < noise_ratio < 0.3:
                        return n_clusters / (noise_ratio + 1)
                return 0.0
            except:
                return 0.0
        
        return objective
    
    def create_svm_objective(self, data: np.ndarray):
        """åˆ›å»ºOne-Class SVMçš„è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        def objective(trial):
            params = {
                'nu': trial.suggest_float('nu', 0.001, 0.5),
                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']) if trial.suggest_categorical('gamma_type', ['auto', 'float']) == 'auto' else trial.suggest_float('gamma_value', 1e-6, 1e-1)
            }
            
            # å¤„ç†gammaå‚æ•°
            if 'gamma_value' in params:
                params['gamma'] = params.pop('gamma_value')
                params.pop('gamma_type', None)
            else:
                params.pop('gamma_type', None)
            
            try:
                model = OneClassSVM(**params)
                model.fit(data)
                scores = -model.decision_function(data)
                return np.std(scores) / (np.mean(scores) + 1e-6)
            except:
                return 0.0
        
        return objective
    
    def optimize_single_model(self, model_name: str, data: np.ndarray, 
                            objective_func, direction: str = 'maximize') -> Dict[str, Any]:
        """ä¼˜åŒ–å•ä¸ªæ¨¡å‹çš„è¶…å‚æ•°"""
        logger.info(f"å¼€å§‹ä¼˜åŒ– {model_name} è¶…å‚æ•°...")
        
        # åˆ›å»ºoptunaç ”ç©¶
        study = optuna.create_study(
            direction=direction,
            sampler=TPESampler(seed=self.random_state),
            pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        start_time = time.time()
        study.optimize(objective_func, n_trials=self.optimization_trials, n_jobs=1)  # optunaå†…éƒ¨å¹¶è¡Œ
        optimization_time = time.time() - start_time
        
        best_params = study.best_params
        best_value = study.best_value
        
        logger.info(f"{model_name} ä¼˜åŒ–å®Œæˆ - æœ€ä½³å€¼: {best_value:.4f}, è€—æ—¶: {optimization_time:.2f}ç§’")
        logger.info(f"{model_name} æœ€ä½³å‚æ•°: {best_params}")
        
        return {
            'best_params': best_params,
            'best_value': best_value,
            'optimization_time': optimization_time,
            'n_trials': len(study.trials)
        }
    
    def train_optimized_model(self, model_name: str, data: np.ndarray, 
                            best_params: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°è®­ç»ƒæ¨¡å‹"""
        start_time = time.time()
        
        try:
            if model_name == 'isolation_forest':
                model = IsolationForest(random_state=self.random_state, n_jobs=self.n_jobs, **best_params)
                model.fit(data)
                scores = -model.decision_function(data)
                
            elif model_name == 'lof':
                model = LocalOutlierFactor(n_jobs=self.n_jobs, **best_params)
                model.fit_predict(data)
                scores = -model.negative_outlier_factor_
                
            elif model_name == 'gmm':
                model = GaussianMixture(random_state=self.random_state, **best_params)
                model.fit(data)
                log_probs = model.score_samples(data)
                scores = -log_probs
                
            elif model_name == 'knn':
                n_neighbors = best_params.get('n_neighbors', 10)
                model = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=self.n_jobs)
                model.fit(data)
                distances, _ = model.kneighbors(data)
                scores = np.mean(distances, axis=1)
                
            elif model_name == 'elliptic_envelope':
                model = EllipticEnvelope(contamination=self.contamination, random_state=self.random_state)
                model.fit(data)
                scores = -model.decision_function(data)
                
            elif model_name == 'dbscan':
                model = DBSCAN(**best_params)
                labels = model.fit_predict(data)
                scores = np.where(labels == -1, 1.0, 0.1)
                
            elif model_name == 'kmeans':
                model = KMeans(random_state=self.random_state, n_init=10, **best_params)
                model.fit(data)
                distances = model.transform(data)
                scores = np.min(distances, axis=1)
                
            elif model_name == 'one_class_svm':
                model = OneClassSVM(**best_params)
                model.fit(data)
                scores = -model.decision_function(data)
                
            elif model_name == 'feature_bagging':
                model = FeatureBagging(
                    contamination=self.contamination,
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                    **best_params
                )
                model.fit(data)
                scores = model.decision_scores_
                
            elif model_name == 'hbos':
                model = HBOS(contamination=self.contamination, **best_params)
                model.fit(data)
                scores = model.decision_scores_
                
            else:
                raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
            
            training_time = time.time() - start_time
            
            return {
                'model': model,
                'scores': scores,
                'params': best_params,
                'training_time': training_time
            }
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ¨¡å‹ {model_name} å¤±è´¥: {e}")
            return None
    
    def train_all_models_parallel(self, processed_data: Dict[str, np.ndarray]) -> Dict[str, Dict]:
        """å¹¶è¡Œè®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        logger.info("å¼€å§‹å¹¶è¡Œè®­ç»ƒæ‰€æœ‰å¼‚å¸¸æ£€æµ‹æ¨¡å‹...")
        
        # æ¨¡å‹é…ç½®ï¼š(æ¨¡å‹å, ä¼˜åŒ–ç›®æ ‡å‡½æ•°åˆ›å»ºå™¨, æ•°æ®ç±»å‹, ä¼˜åŒ–æ–¹å‘)
        model_configs = [
            ('isolation_forest', self.create_isolation_forest_objective, 'standard', 'maximize'),
            ('lof', self.create_lof_objective, 'standard', 'maximize'),
            ('gmm', self.create_gmm_objective, 'standard', 'maximize'),
            ('knn', None, 'standard', None),  # KNNä¸éœ€è¦ä¼˜åŒ–
            ('elliptic_envelope', None, 'robust', None),  # Elliptic Envelopeä¸éœ€è¦ä¼˜åŒ–
            ('dbscan', self.create_dbscan_objective, 'minmax', 'maximize'),
            ('kmeans', self.create_kmeans_objective, 'standard', 'maximize'),
            ('one_class_svm', self.create_svm_objective, 'standard', 'maximize'),
            ('feature_bagging', None, 'minmax', None),  # Feature Baggingä½¿ç”¨é»˜è®¤å‚æ•°
            ('hbos', None, 'minmax', None),  # HBOSä½¿ç”¨é»˜è®¤å‚æ•°
        ]
        
        results = {}
        optimization_results = {}
        
        for model_name, objective_creator, data_type, direction in model_configs:
            logger.info(f"å¤„ç†æ¨¡å‹: {model_name}")
            data = processed_data[data_type]
            
            # è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if objective_creator is not None:
                objective_func = objective_creator(data)
                opt_result = self.optimize_single_model(model_name, data, objective_func, direction)
                optimization_results[model_name] = opt_result
                best_params = opt_result['best_params']
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°
                if model_name == 'knn':
                    best_params = {'n_neighbors': 10}
                elif model_name == 'feature_bagging':
                    best_params = {'n_estimators': 10}
                elif model_name == 'hbos':
                    best_params = {'n_bins': 20, 'alpha': 0.1}
                else:
                    best_params = {}
                optimization_results[model_name] = {'best_params': best_params, 'optimization_time': 0}
            
            # è®­ç»ƒæ¨¡å‹
            result = self.train_optimized_model(model_name, data, best_params)
            if result:
                results[model_name] = result
                logger.info(f"{model_name} è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {result['training_time']:.2f}ç§’")
        
        # ä¿å­˜ä¼˜åŒ–ç»“æœ
        self.best_params = {name: res['best_params'] for name, res in optimization_results.items()}
        
        # ç®€åŒ–ç»Ÿè®¡è¾“å‡º
        pass
        
        logger.info(f"æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼æˆåŠŸè®­ç»ƒ {len(results)} ä¸ªæ¨¡å‹")
        return results
    
    def save_models_and_results(self, results: Dict[str, Dict]):
        """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
        logger.info("ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
        
        # ä¿å­˜æ¨¡å‹
        for model_name, result in results.items():
            model_path = self.model_dir / f'optimized_{model_name}_model.joblib'
            joblib.dump(result['model'], model_path)
        
        # ç”Ÿæˆåˆ†æ•°è¡¨
        scores_data = {'loop_id': self.loop_ids}
        
        for model_name, result in results.items():
            scores = result['scores']
            # æ ‡å‡†åŒ–åˆ†æ•°åˆ°0-1èŒƒå›´
            scores_normalized = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))
            scores_data[f'{model_name}_score'] = scores_normalized
            scores_data[f'{model_name}_rank'] = len(scores) - np.argsort(np.argsort(scores))
        
        scores_df = pd.DataFrame(scores_data)
        output_file = self.output_dir / 'optimized_model_scores.csv'
        scores_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"æ¨¡å‹å’Œç»“æœå·²ä¿å­˜")
        
        # ç®€åŒ–æŠ¥å‘Šç”Ÿæˆ
        pass
        
        return scores_df
    
    def generate_training_report(self, results: Dict[str, Dict], scores_df: pd.DataFrame):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        model_stats = {}
        
        for model_name, result in results.items():
            scores = result['scores']
            model_stats[model_name] = {
                'mean_score': float(np.mean(scores)),
                'std_score': float(np.std(scores)),
                'min_score': float(np.min(scores)),
                'max_score': float(np.max(scores)),
                'training_time': result['training_time'],
                'optimized_parameters': result['params'],
                'top_1_percent_threshold': float(np.percentile(scores, 99)),
                'top_5_percent_threshold': float(np.percentile(scores, 95))
            }
        
        report = {
            'summary': {
                'total_loops': len(self.loop_ids),
                'total_features': len(self.feature_names),
                'total_models': len(results),
                'contamination_rate': self.contamination,
                'cpu_cores_used': self.n_jobs,
                'optimization_trials_per_model': self.optimization_trials
            },
            'feature_names': self.feature_names,
            'model_statistics': model_stats,
            'best_parameters': self.best_params
        }
        
        report_file = self.output_dir / 'optimized_training_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    def run_complete_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹æµç¨‹"""
        logger.info("å¼€å§‹è¿è¡Œä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹æµç¨‹...")
        start_time = time.time()
        
        # 1. åŠ è½½ç‰¹å¾æ•°æ®
        if not self.load_engineered_features():
            return False
        
        # 2. æ•°æ®é¢„å¤„ç†
        processed_data = self.preprocess_data()
        
        # 3. å¹¶è¡Œè®­ç»ƒæ¨¡å‹
        results = self.train_all_models_parallel(processed_data)
        
        if not results:
            logger.error("æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ¨¡å‹")
            return False
        
        # 4. ä¿å­˜ç»“æœ
        scores_df = self.save_models_and_results(results)
        
        total_time = time.time() - start_time
        
        logger.info(f"ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹æµç¨‹å®Œæˆï¼")
        logger.info(f"æˆåŠŸè®­ç»ƒ {len(results)} ä¸ªä¼˜åŒ–æ¨¡å‹")
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"å¹³å‡æ¯ä¸ªæ¨¡å‹: {total_time/len(results):.2f}ç§’")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("åŒ–å·¥å›¾é£æ§ç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹æ¨¡å‹å±‚")
    print("ğŸš€ è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ– + å¤šCPUå¹¶è¡Œè®­ç»ƒ")
    print("=" * 70)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æˆçš„ç‰¹å¾æ•°æ®
    feature_file = "outputs/anomaly_detection/features/engineered_features.csv"
    if not Path(feature_file).exists():
        print("âŒ æ‰¾ä¸åˆ°ç‰¹å¾å·¥ç¨‹è¾“å‡ºæ–‡ä»¶")
        print("ğŸ“ è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹:")
        print("   python feature_engineering.py")
        return
    
    # åˆå§‹åŒ–ä¼˜åŒ–æ£€æµ‹å™¨
    detector = OptimizedAnomalyDetection(
        contamination=0.05,
        random_state=42,
        n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        optimization_trials=30  # æ¯ä¸ªæ¨¡å‹30æ¬¡ä¼˜åŒ–è¯•éªŒï¼Œå¤Ÿç”¨ä¸”å¿«é€Ÿ
    )
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = detector.run_complete_pipeline()
    
    if success:
        print("\n" + "=" * 70)
        print("âœ… ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹å®Œæˆï¼")
        print("ğŸ“Š è¾“å‡ºæ–‡ä»¶:")
        print("   - optimized_model_scores.csv (æ¨¡å‹åˆ†æ•°)")
        print("   - optimized_training_report.json (è®­ç»ƒæŠ¥å‘Š)")
        print("   - optimization_stats.json (ä¼˜åŒ–ç»Ÿè®¡)")
        print("ğŸ“ è¾“å‡ºç›®å½•: outputs/anomaly_detection/")
        print("=" * 70)
    else:
        print("âŒ å¼‚å¸¸æ£€æµ‹æµç¨‹å¤±è´¥")

if __name__ == "__main__":
    main() 