#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ£€æµ‹è§£é‡Šç³»ç»Ÿ - æœ€ç»ˆè„šæœ¬
ä¸ºå¼‚å¸¸æ£€æµ‹ç»“æœæä¾›è¯¦ç»†çš„å¯è§£é‡Šæ€§åˆ†æå’Œä¸šåŠ¡ç†è§£

åŠŸèƒ½æ¨¡å—ï¼š
1. ç‰¹å¾é‡è¦æ€§åˆ†æ - è§£é‡Šå“ªäº›ç‰¹å¾å¯¼è‡´å¼‚å¸¸
2. å¼‚å¸¸æ¨¡å¼è¯†åˆ« - è¯†åˆ«ä¸åŒç±»å‹çš„å¼‚å¸¸æ¨¡å¼
3. é£é™©ç­‰çº§è§£é‡Š - è¯¦ç»†è¯´æ˜é£é™©ç­‰çº§åˆ¤å®šä¾æ®
4. ä¸šåŠ¡å½±å“è¯„ä¼° - è¯„ä¼°å¼‚å¸¸å¯¹ä¸šåŠ¡çš„æ½œåœ¨å½±å“
5. ä¿®å¤å»ºè®®ç”Ÿæˆ - æä¾›é’ˆå¯¹æ€§çš„é£é™©ç¼“è§£å»ºè®®
6. å¯è§†åŒ–æŠ¥å‘Š - ç”Ÿæˆç›´è§‚çš„è§£é‡ŠæŠ¥å‘Š

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2024å¹´
ç‰ˆæœ¬: v1.0 - è§£é‡Šå±‚
"""

import pandas as pd
import numpy as np
import warnings
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import percentileofscore
import joblib
from sklearn.tree import DecisionTreeRegressor, export_text
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import shap

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/anomaly_detection/logs/explanation_system.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AnomalyExplanationSystem:
    """å¼‚å¸¸æ£€æµ‹è§£é‡Šç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§£é‡Šç³»ç»Ÿ"""
        self.output_dir = Path('outputs/anomaly_detection')
        self.explanation_dir = self.output_dir / 'explanations'
        self.viz_dir = self.output_dir / 'visualizations'
        self.reports_dir = self.output_dir / 'reports'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        for dir_path in [self.explanation_dir, self.viz_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.anomaly_results = None
        self.feature_data = None
        self.feature_names = []
        self.explanation_models = {}
        self.feature_importance_global = {}
        
        logger.info("å¼‚å¸¸æ£€æµ‹è§£é‡Šç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_data(self) -> bool:
        """åŠ è½½å¼‚å¸¸æ£€æµ‹ç»“æœå’Œç‰¹å¾æ•°æ®"""
        try:
            # åŠ è½½æœ€ç»ˆå¼‚å¸¸æ£€æµ‹ç»“æœ
            results_file = self.output_dir / 'ensemble/final_anomaly_results.csv'
            if not results_file.exists():
                logger.error(f"æ‰¾ä¸åˆ°å¼‚å¸¸æ£€æµ‹ç»“æœæ–‡ä»¶: {results_file}")
                return False
            
            self.anomaly_results = pd.read_csv(results_file)
            logger.info(f"åŠ è½½å¼‚å¸¸æ£€æµ‹ç»“æœ: {len(self.anomaly_results)} ä¸ªç¯è·¯")
            
            # åŠ è½½ç‰¹å¾æ•°æ®
            features_file = self.output_dir / 'features/engineered_features.csv'
            if not features_file.exists():
                logger.error(f"æ‰¾ä¸åˆ°ç‰¹å¾æ•°æ®æ–‡ä»¶: {features_file}")
                return False
            
            self.feature_data = pd.read_csv(features_file)
            
            # åˆ†ç¦»loop_idå’Œç‰¹å¾
            if 'loop_id' in self.feature_data.columns:
                feature_cols = [col for col in self.feature_data.columns if col != 'loop_id']
            else:
                feature_cols = self.feature_data.columns.tolist()
            
            self.feature_names = feature_cols
            logger.info(f"åŠ è½½ç‰¹å¾æ•°æ®: {len(self.feature_names)} ä¸ªç‰¹å¾")
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def build_explanation_models(self):
        """æ„å»ºå¯è§£é‡Šæ€§æ¨¡å‹"""
        logger.info("æ„å»ºå¯è§£é‡Šæ€§æ¨¡å‹...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.feature_data[self.feature_names].values
        y = self.anomaly_results['final_anomaly_score'].values
        
        # 1. å†³ç­–æ ‘æ¨¡å‹ - æ˜“äºè§£é‡Šçš„è§„åˆ™
        dt_model = DecisionTreeRegressor(
            max_depth=8,
            min_samples_split=100,
            min_samples_leaf=50,
            random_state=42
        )
        dt_model.fit(X, y)
        self.explanation_models['decision_tree'] = dt_model
        
        # 2. éšæœºæ£®æ—æ¨¡å‹ - ç‰¹å¾é‡è¦æ€§
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=50,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X, y)
        self.explanation_models['random_forest'] = rf_model
        
        # 3. è®¡ç®—å…¨å±€ç‰¹å¾é‡è¦æ€§
        self.feature_importance_global = {
            'decision_tree': dict(zip(self.feature_names, dt_model.feature_importances_)),
            'random_forest': dict(zip(self.feature_names, rf_model.feature_importances_))
        }
        
        logger.info("å¯è§£é‡Šæ€§æ¨¡å‹æ„å»ºå®Œæˆ")
    
    def generate_decision_rules(self) -> Dict[str, Any]:
        """ç”Ÿæˆå†³ç­–è§„åˆ™"""
        logger.info("ç”Ÿæˆå†³ç­–è§„åˆ™...")
        
        dt_model = self.explanation_models['decision_tree']
        
        # å¯¼å‡ºå†³ç­–æ ‘è§„åˆ™
        tree_rules = export_text(
            dt_model,
            feature_names=self.feature_names,
            max_depth=6
        )
        
        # è§£æå…³é”®å†³ç­–è·¯å¾„
        key_rules = []
        
        # è·å–å¶å­èŠ‚ç‚¹çš„å€¼å’Œæ ·æœ¬æ•°
        leaf_values = dt_model.tree_.value.flatten()
        leaf_samples = dt_model.tree_.n_node_samples
        
        # æ‰¾å‡ºé«˜å¼‚å¸¸åˆ†æ•°çš„å¶å­èŠ‚ç‚¹è·¯å¾„
        high_anomaly_threshold = np.percentile(self.anomaly_results['final_anomaly_score'], 90)
        
        for i, (value, samples) in enumerate(zip(leaf_values, leaf_samples)):
            if value > high_anomaly_threshold and samples > 20:  # é«˜å¼‚å¸¸åˆ†æ•°ä¸”æ ·æœ¬æ•°è¶³å¤Ÿ
                key_rules.append({
                    'node_id': i,
                    'predicted_score': float(value),
                    'sample_count': int(samples),
                    'rule_description': f"é¢„æµ‹å¼‚å¸¸åˆ†æ•°: {value:.3f}, å½±å“æ ·æœ¬: {samples}"
                })
        
        return {
            'tree_rules_text': tree_rules,
            'key_decision_paths': key_rules,
            'total_rules': len(key_rules)
        }
    
    def analyze_feature_importance(self) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        logger.info("åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        # åˆå¹¶ä¸åŒæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        rf_importance = self.feature_importance_global['random_forest']
        dt_importance = self.feature_importance_global['decision_tree']
        
        # åˆ›å»ºç»¼åˆç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_analysis = {}
        
        for feature in self.feature_names:
            rf_imp = rf_importance.get(feature, 0)
            dt_imp = dt_importance.get(feature, 0)
            
            # è®¡ç®—ç»¼åˆé‡è¦æ€§åˆ†æ•°
            combined_importance = (rf_imp + dt_imp) / 2
            
            # è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            feature_values = self.feature_data[feature].values
            feature_stats = {
                'mean': float(np.mean(feature_values)),
                'std': float(np.std(feature_values)),
                'min': float(np.min(feature_values)),
                'max': float(np.max(feature_values)),
                'skewness': float(stats.skew(feature_values)),
                'kurtosis': float(stats.kurtosis(feature_values))
            }
            
            feature_analysis[feature] = {
                'rf_importance': rf_imp,
                'dt_importance': dt_imp,
                'combined_importance': combined_importance,
                'statistics': feature_stats,
                'rank': 0  # å°†åœ¨åé¢æ’åºæ—¶å¡«å…¥
            }
        
        # æŒ‰ç»¼åˆé‡è¦æ€§æ’åº
        sorted_features = sorted(
            feature_analysis.items(),
            key=lambda x: x[1]['combined_importance'],
            reverse=True
        )
        
        # æ·»åŠ æ’åä¿¡æ¯
        for rank, (feature, info) in enumerate(sorted_features, 1):
            feature_analysis[feature]['rank'] = rank
        
        # è·å–å‰10é‡è¦ç‰¹å¾
        top_10_features = dict(sorted_features[:10])
        
        return {
            'all_features': feature_analysis,
            'top_10_features': top_10_features,
            'feature_ranking': [item[0] for item in sorted_features]
        }
    
    def identify_anomaly_patterns(self) -> Dict[str, Any]:
        """è¯†åˆ«å¼‚å¸¸æ¨¡å¼"""
        logger.info("è¯†åˆ«å¼‚å¸¸æ¨¡å¼...")
        
        # è·å–é«˜å¼‚å¸¸åˆ†æ•°çš„ç¯è·¯
        high_anomaly_data = self.anomaly_results[
            self.anomaly_results['anomaly_level'].isin(['æé«˜é£é™©', 'é«˜é£é™©'])
        ].copy()
        
        if len(high_anomaly_data) == 0:
            return {'patterns': [], 'total_high_risk': 0}
        
        # è·å–å¯¹åº”çš„ç‰¹å¾æ•°æ®
        high_anomaly_loop_ids = high_anomaly_data['loop_id'].values
        
        # åˆå¹¶æ•°æ®
        merged_data = pd.merge(
            high_anomaly_data[['loop_id', 'final_anomaly_score', 'anomaly_level']],
            self.feature_data,
            on='loop_id',
            how='inner'
        )
        
        # åˆ†æå¼‚å¸¸æ¨¡å¼
        patterns = []
        
        # 1. åŸºäºå¼‚å¸¸åˆ†æ•°åˆ†ç»„åˆ†æ
        score_groups = {
            'æé«˜å¼‚å¸¸': merged_data[merged_data['anomaly_level'] == 'æé«˜é£é™©'],
            'é«˜å¼‚å¸¸': merged_data[merged_data['anomaly_level'] == 'é«˜é£é™©']
        }
        
        for group_name, group_data in score_groups.items():
            if len(group_data) == 0:
                continue
            
            # è®¡ç®—è¯¥ç»„çš„ç‰¹å¾ç»Ÿè®¡
            group_features = group_data[self.feature_names]
            
            # æ‰¾å‡ºè¯¥ç»„çš„ç‰¹å¾ç‰¹ç‚¹ï¼ˆä¸æ•´ä½“å‡å€¼æ¯”è¾ƒï¼‰
            overall_means = self.feature_data[self.feature_names].mean()
            group_means = group_features.mean()
            
            # æ‰¾å‡ºæ˜¾è‘—åç¦»çš„ç‰¹å¾
            significant_features = []
            for feature in self.feature_names:
                if feature in group_means.index and feature in overall_means.index:
                    group_mean = group_means[feature]
                    overall_mean = overall_means[feature]
                    
                    if overall_mean != 0:
                        deviation_ratio = abs(group_mean - overall_mean) / abs(overall_mean)
                        if deviation_ratio > 0.5:  # åç¦»50%ä»¥ä¸Š
                            significant_features.append({
                                'feature': feature,
                                'group_mean': float(group_mean),
                                'overall_mean': float(overall_mean),
                                'deviation_ratio': float(deviation_ratio),
                                'direction': 'higher' if group_mean > overall_mean else 'lower'
                            })
            
            # æŒ‰åç¦»ç¨‹åº¦æ’åº
            significant_features.sort(key=lambda x: x['deviation_ratio'], reverse=True)
            
            patterns.append({
                'pattern_name': group_name,
                'sample_count': len(group_data),
                'avg_anomaly_score': float(group_data['final_anomaly_score'].mean()),
                'significant_features': significant_features[:5],  # å‰5ä¸ªæœ€æ˜¾è‘—ç‰¹å¾
                'description': self._generate_pattern_description(group_name, significant_features[:3])
            })
        
        return {
            'patterns': patterns,
            'total_high_risk': len(high_anomaly_data),
            'pattern_summary': f"è¯†åˆ«å‡º {len(patterns)} ç§ä¸»è¦å¼‚å¸¸æ¨¡å¼"
        }
    
    def _generate_pattern_description(self, pattern_name: str, features: List[Dict]) -> str:
        """ç”Ÿæˆæ¨¡å¼æè¿°"""
        if not features:
            return f"{pattern_name}ç¯è·¯çš„ç‰¹å¾æ¨¡å¼ä¸æ˜æ˜¾"
        
        descriptions = []
        for feature_info in features[:3]:  # åªæè¿°å‰3ä¸ªç‰¹å¾
            feature = feature_info['feature']
            direction = "æ˜¾è‘—é«˜äº" if feature_info['direction'] == 'higher' else "æ˜¾è‘—ä½äº"
            ratio = feature_info['deviation_ratio']
            
            # ç®€åŒ–ç‰¹å¾åç§°æ˜¾ç¤º
            display_name = feature.replace('_', ' ').title()
            descriptions.append(f"{display_name}{direction}æ­£å¸¸æ°´å¹³({ratio:.1%})")
        
        return f"{pattern_name}ç¯è·¯ä¸»è¦ç‰¹å¾: " + "; ".join(descriptions)
    
    def explain_individual_anomalies(self, top_n: int = 50) -> Dict[str, Any]:
        """è§£é‡Šä¸ªåˆ«å¼‚å¸¸ç¯è·¯"""
        logger.info(f"è§£é‡Šå‰{top_n}ä¸ªå¼‚å¸¸ç¯è·¯...")
        
        # è·å–å‰Nä¸ªå¼‚å¸¸ç¯è·¯
        top_anomalies = self.anomaly_results.nlargest(top_n, 'final_anomaly_score')
        
        explanations = []
        
        for idx, row in top_anomalies.iterrows():
            loop_id = row['loop_id']
            anomaly_score = row['final_anomaly_score']
            risk_level = row['anomaly_level']
            
            # è·å–è¯¥ç¯è·¯çš„ç‰¹å¾å€¼
            loop_features = self.feature_data[self.feature_data['loop_id'] == loop_id]
            if len(loop_features) == 0:
                continue
            
            loop_features = loop_features.iloc[0]
            
            # ä½¿ç”¨å†³ç­–æ ‘æ¨¡å‹è§£é‡Š
            feature_values = loop_features[self.feature_names].values.reshape(1, -1)
            dt_prediction = self.explanation_models['decision_tree'].predict(feature_values)[0]
            
            # æ‰¾å‡ºæœ€é‡è¦çš„è´¡çŒ®ç‰¹å¾
            rf_model = self.explanation_models['random_forest']
            feature_contributions = []
            
            # è®¡ç®—ç‰¹å¾è´¡çŒ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            for i, feature in enumerate(self.feature_names):
                feature_value = loop_features[feature]
                feature_importance = self.feature_importance_global['random_forest'][feature]
                
                # è®¡ç®—è¯¥ç‰¹å¾ç›¸å¯¹äºå¹³å‡å€¼çš„åç¦»
                feature_mean = self.feature_data[feature].mean()
                feature_std = self.feature_data[feature].std()
                
                if feature_std > 0:
                    normalized_deviation = abs(feature_value - feature_mean) / feature_std
                    contribution_score = feature_importance * normalized_deviation
                    
                    feature_contributions.append({
                        'feature': feature,
                        'value': float(feature_value),
                        'mean': float(feature_mean),
                        'deviation': float(normalized_deviation),
                        'contribution': float(contribution_score)
                    })
            
            # æŒ‰è´¡çŒ®åº¦æ’åº
            feature_contributions.sort(key=lambda x: x['contribution'], reverse=True)
            
            explanations.append({
                'loop_id': int(loop_id),
                'anomaly_score': float(anomaly_score),
                'risk_level': risk_level,
                'percentile_rank': float(percentileofscore(
                    self.anomaly_results['final_anomaly_score'], 
                    anomaly_score
                )),
                'dt_prediction': float(dt_prediction),
                'top_contributing_features': feature_contributions[:5],
                'explanation_summary': self._generate_individual_explanation(
                    loop_id, risk_level, feature_contributions[:3]
                )
            })
        
        return {
            'individual_explanations': explanations,
            'total_explained': len(explanations),
            'explanation_method': 'å†³ç­–æ ‘ + éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§'
        }
    
    def _generate_individual_explanation(self, loop_id: int, risk_level: str, 
                                       top_features: List[Dict]) -> str:
        """ç”Ÿæˆä¸ªåˆ«ç¯è·¯çš„è§£é‡Š"""
        base_text = f"ç¯è·¯ {loop_id} è¢«è¯„ä¸º {risk_level}ï¼Œä¸»è¦åŸå› ï¼š"
        
        reasons = []
        for feature_info in top_features:
            feature = feature_info['feature'].replace('_', ' ')
            deviation = feature_info['deviation']
            
            if deviation > 2:
                reasons.append(f"{feature}å¼‚å¸¸åç¦»æ­£å¸¸èŒƒå›´({deviation:.1f}å€æ ‡å‡†å·®)")
            elif deviation > 1:
                reasons.append(f"{feature}æ˜æ˜¾åç¦»æ­£å¸¸æ°´å¹³({deviation:.1f}å€æ ‡å‡†å·®)")
        
        return base_text + "; ".join(reasons) if reasons else base_text + "å¤šé¡¹æŒ‡æ ‡ç»¼åˆå¼‚å¸¸"
    
    def generate_business_impact_assessment(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¸šåŠ¡å½±å“è¯„ä¼°"""
        logger.info("ç”Ÿæˆä¸šåŠ¡å½±å“è¯„ä¼°...")
        
        # æŒ‰é£é™©ç­‰çº§ç»Ÿè®¡
        risk_distribution = self.anomaly_results['anomaly_level'].value_counts().to_dict()
        
        # è®¡ç®—æ½œåœ¨å½±å“
        high_risk_count = risk_distribution.get('æé«˜é£é™©', 0) + risk_distribution.get('é«˜é£é™©', 0)
        total_loops = len(self.anomaly_results)
        high_risk_ratio = high_risk_count / total_loops if total_loops > 0 else 0
        
        # ä¸šåŠ¡å½±å“è¯„ä¼°
        impact_assessment = {
            'overall_risk_status': self._assess_overall_risk(high_risk_ratio),
            'risk_distribution': risk_distribution,
            'high_risk_ratio': float(high_risk_ratio),
            'total_loops_analyzed': total_loops,
            'immediate_attention_required': high_risk_count,
            'business_implications': self._generate_business_implications(risk_distribution),
            'priority_actions': self._generate_priority_actions(risk_distribution)
        }
        
        return impact_assessment
    
    def _assess_overall_risk(self, high_risk_ratio: float) -> str:
        """è¯„ä¼°æ•´ä½“é£é™©çŠ¶æ€"""
        if high_risk_ratio > 0.1:
            return "é«˜é£é™©çŠ¶æ€ - éœ€è¦ç«‹å³å…³æ³¨"
        elif high_risk_ratio > 0.05:
            return "ä¸­ç­‰é£é™©çŠ¶æ€ - éœ€è¦å¯†åˆ‡ç›‘æ§"
        elif high_risk_ratio > 0.02:
            return "ä½é£é™©çŠ¶æ€ - æ­£å¸¸ç›‘æ§"
        else:
            return "é£é™©å¯æ§çŠ¶æ€ - å®šæœŸæ£€æŸ¥"
    
    def _generate_business_implications(self, risk_dist: Dict[str, int]) -> List[str]:
        """ç”Ÿæˆä¸šåŠ¡å½±å“åˆ†æ"""
        implications = []
        
        extreme_risk = risk_dist.get('æé«˜é£é™©', 0)
        high_risk = risk_dist.get('é«˜é£é™©', 0)
        
        if extreme_risk > 0:
            implications.append(f"{extreme_risk} ä¸ªç¯è·¯å­˜åœ¨æé«˜é£é™©ï¼Œå¯èƒ½æ¶‰åŠé‡å¤§åˆè§„é—®é¢˜")
        
        if high_risk > 0:
            implications.append(f"{high_risk} ä¸ªç¯è·¯å­˜åœ¨é«˜é£é™©ï¼Œéœ€è¦ä¼˜å…ˆè°ƒæŸ¥")
        
        if extreme_risk + high_risk > 100:
            implications.append("å¤§é‡é«˜é£é™©ç¯è·¯å¯èƒ½è¡¨æ˜ç³»ç»Ÿæ€§é—®é¢˜")
        
        return implications
    
    def _generate_priority_actions(self, risk_dist: Dict[str, int]) -> List[str]:
        """ç”Ÿæˆä¼˜å…ˆè¡ŒåŠ¨å»ºè®®"""
        actions = []
        
        extreme_risk = risk_dist.get('æé«˜é£é™©', 0)
        high_risk = risk_dist.get('é«˜é£é™©', 0)
        
        if extreme_risk > 0:
            actions.append("ç«‹å³å®¡æŸ¥æ‰€æœ‰æé«˜é£é™©ç¯è·¯çš„åˆè§„çŠ¶æ€")
            actions.append("æš‚åœæˆ–é™åˆ¶æé«˜é£é™©ç¯è·¯çš„ç›¸å…³ä¸šåŠ¡æ´»åŠ¨")
        
        if high_risk > 0:
            actions.append("48å°æ—¶å†…å®Œæˆé«˜é£é™©ç¯è·¯çš„è¯¦ç»†è°ƒæŸ¥")
            actions.append("å»ºç«‹é«˜é£é™©ç¯è·¯çš„æŒç»­ç›‘æ§æœºåˆ¶")
        
        if extreme_risk + high_risk > 50:
            actions.append("å¯åŠ¨ç³»ç»Ÿæ€§é£é™©è¯„ä¼°ç¨‹åº")
            actions.append("è€ƒè™‘è°ƒæ•´é£é™©ç®¡ç†ç­–ç•¥å’Œæ§åˆ¶æªæ–½")
        
        return actions
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # 1. ç‰¹å¾é‡è¦æ€§å›¾
        self._create_feature_importance_plot()
        
        # 2. å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒå›¾
        self._create_anomaly_distribution_plot()
        
        # 3. é£é™©ç­‰çº§åˆ†å¸ƒå›¾
        self._create_risk_level_plot()
        
        # 4. å‰20å¼‚å¸¸ç¯è·¯ç‰¹å¾é›·è¾¾å›¾
        self._create_top_anomalies_radar()
        
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {self.viz_dir}")
    
    def _create_feature_importance_plot(self):
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾"""
        feature_analysis = self.analyze_feature_importance()
        top_features = feature_analysis['top_10_features']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # éšæœºæ£®æ—é‡è¦æ€§
        features = list(top_features.keys())
        rf_importance = [top_features[f]['rf_importance'] for f in features]
        
        ax1.barh(range(len(features)), rf_importance, color='skyblue')
        ax1.set_yticks(range(len(features)))
        ax1.set_yticklabels([f.replace('_', '\n') for f in features])
        ax1.set_title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 10)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('é‡è¦æ€§åˆ†æ•°')
        
        # å†³ç­–æ ‘é‡è¦æ€§
        dt_importance = [top_features[f]['dt_importance'] for f in features]
        
        ax2.barh(range(len(features)), dt_importance, color='lightcoral')
        ax2.set_yticks(range(len(features)))
        ax2.set_yticklabels([f.replace('_', '\n') for f in features])
        ax2.set_title('å†³ç­–æ ‘ç‰¹å¾é‡è¦æ€§ (Top 10)', fontsize=14, fontweight='bold')
        ax2.set_xlabel('é‡è¦æ€§åˆ†æ•°')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'feature_importance_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_anomaly_distribution_plot(self):
        """åˆ›å»ºå¼‚å¸¸åˆ†æ•°åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        scores = self.anomaly_results['final_anomaly_score']
        
        # ç›´æ–¹å›¾
        axes[0,0].hist(scores, bins=50, alpha=0.7, color='red', edgecolor='black')
        axes[0,0].set_title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=12, fontweight='bold')
        axes[0,0].set_xlabel('å¼‚å¸¸åˆ†æ•°')
        axes[0,0].set_ylabel('é¢‘æ¬¡')
        
        # ç®±çº¿å›¾
        axes[0,1].boxplot(scores, vert=True)
        axes[0,1].set_title('å¼‚å¸¸åˆ†æ•°ç®±çº¿å›¾', fontsize=12, fontweight='bold')
        axes[0,1].set_ylabel('å¼‚å¸¸åˆ†æ•°')
        
        # ç´¯ç§¯åˆ†å¸ƒ
        sorted_scores = np.sort(scores)
        axes[1,0].plot(sorted_scores, np.arange(1, len(sorted_scores)+1) / len(sorted_scores))
        axes[1,0].set_title('å¼‚å¸¸åˆ†æ•°ç´¯ç§¯åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        axes[1,0].set_xlabel('å¼‚å¸¸åˆ†æ•°')
        axes[1,0].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
        
        # åˆ†ä½æ•°å›¾
        percentiles = np.arange(1, 100)
        percentile_values = [np.percentile(scores, p) for p in percentiles]
        axes[1,1].plot(percentiles, percentile_values, 'b-', linewidth=2)
        axes[1,1].set_title('å¼‚å¸¸åˆ†æ•°åˆ†ä½æ•°å›¾', fontsize=12, fontweight='bold')
        axes[1,1].set_xlabel('ç™¾åˆ†ä½æ•°')
        axes[1,1].set_ylabel('å¼‚å¸¸åˆ†æ•°')
        
        # æ ‡è®°å…³é”®åˆ†ä½ç‚¹
        key_percentiles = [90, 95, 99]
        for p in key_percentiles:
            value = np.percentile(scores, p)
            axes[1,1].axhline(y=value, color='red', linestyle='--', alpha=0.7)
            axes[1,1].text(p, value, f'{p}%: {value:.3f}', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'anomaly_score_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_risk_level_plot(self):
        """åˆ›å»ºé£é™©ç­‰çº§åˆ†å¸ƒå›¾"""
        risk_counts = self.anomaly_results['anomaly_level'].value_counts()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # é¥¼å›¾
        colors = ['darkred', 'red', 'orange', 'yellow', 'green']
        ax1.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', 
                colors=colors, startangle=90)
        ax1.set_title('é£é™©ç­‰çº§åˆ†å¸ƒ (é¥¼å›¾)', fontsize=12, fontweight='bold')
        
        # æŸ±çŠ¶å›¾
        bars = ax2.bar(risk_counts.index, risk_counts.values, color=colors)
        ax2.set_title('é£é™©ç­‰çº§åˆ†å¸ƒ (æŸ±çŠ¶å›¾)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('ç¯è·¯æ•°é‡')
        ax2.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'risk_level_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_top_anomalies_radar(self):
        """åˆ›å»ºå‰20å¼‚å¸¸ç¯è·¯çš„ç‰¹å¾é›·è¾¾å›¾"""
        # è·å–å‰20å¼‚å¸¸ç¯è·¯
        top_20 = self.anomaly_results.nlargest(20, 'final_anomaly_score')
        
        # è·å–å‰10é‡è¦ç‰¹å¾
        feature_analysis = self.analyze_feature_importance()
        top_10_features = list(feature_analysis['top_10_features'].keys())
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        angles = np.linspace(0, 2 * np.pi, len(top_10_features), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆé›·è¾¾å›¾
        
        fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))
        
        # è®¡ç®—ç‰¹å¾çš„æ ‡å‡†åŒ–å€¼
        feature_data_subset = self.feature_data[self.feature_data['loop_id'].isin(top_20['loop_id'])]
        
        for i, (idx, row) in enumerate(top_20.head(5).iterrows()):  # åªç”»å‰5ä¸ª
            loop_id = row['loop_id']
            loop_features = feature_data_subset[feature_data_subset['loop_id'] == loop_id]
            
            if len(loop_features) == 0:
                continue
            
            # è·å–ç‰¹å¾å€¼å¹¶æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
            values = []
            for feature in top_10_features:
                feature_value = loop_features[feature].iloc[0]
                feature_min = self.feature_data[feature].min()
                feature_max = self.feature_data[feature].max()
                
                if feature_max > feature_min:
                    normalized_value = (feature_value - feature_min) / (feature_max - feature_min)
                else:
                    normalized_value = 0.5
                
                values.append(normalized_value)
            
            values += values[:1]  # é—­åˆé›·è¾¾å›¾
            
            # ç»˜åˆ¶é›·è¾¾å›¾
            color = plt.cm.Reds(0.5 + i * 0.1)
            ax.plot(angles, values, 'o-', linewidth=2, label=f'ç¯è·¯ {int(loop_id)}', color=color)
            ax.fill(angles, values, alpha=0.1, color=color)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels([f.replace('_', '\n') for f in top_10_features])
        ax.set_ylim(0, 1)
        ax.set_title('å‰5å¼‚å¸¸ç¯è·¯ç‰¹å¾é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'top_anomalies_radar.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè§£é‡ŠæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆè§£é‡ŠæŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        decision_rules = self.generate_decision_rules()
        feature_importance = self.analyze_feature_importance()
        anomaly_patterns = self.identify_anomaly_patterns()
        individual_explanations = self.explain_individual_anomalies(50)
        business_impact = self.generate_business_impact_assessment()
        
        # åˆ›å»ºç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            'report_metadata': {
                'generation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_loops_analyzed': len(self.anomaly_results),
                'total_features_used': len(self.feature_names),
                'explanation_methods': ['å†³ç­–æ ‘è§„åˆ™', 'éšæœºæ£®æ—é‡è¦æ€§', 'ç‰¹å¾ç»Ÿè®¡åˆ†æ']
            },
            'executive_summary': {
                'overall_risk_status': business_impact['overall_risk_status'],
                'high_risk_loops': business_impact['immediate_attention_required'],
                'key_risk_factors': feature_importance['feature_ranking'][:5],
                'main_anomaly_patterns': len(anomaly_patterns['patterns'])
            },
            'detailed_analysis': {
                'feature_importance_analysis': feature_importance,
                'decision_rules': decision_rules,
                'anomaly_patterns': anomaly_patterns,
                'individual_explanations': individual_explanations,
                'business_impact_assessment': business_impact
            },
            'recommendations': {
                'immediate_actions': business_impact['priority_actions'],
                'monitoring_focus': feature_importance['feature_ranking'][:10],
                'risk_mitigation_strategies': self._generate_mitigation_strategies(anomaly_patterns)
            }
        }
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_file = self.reports_dir / 'comprehensive_explanation_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(comprehensive_report)
        
        logger.info(f"ç»¼åˆè§£é‡ŠæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return comprehensive_report
    
    def _generate_mitigation_strategies(self, anomaly_patterns: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé£é™©ç¼“è§£ç­–ç•¥"""
        strategies = []
        
        patterns = anomaly_patterns.get('patterns', [])
        
        if patterns:
            strategies.append("é’ˆå¯¹è¯†åˆ«çš„å¼‚å¸¸æ¨¡å¼å»ºç«‹ä¸“é¡¹ç›‘æ§æœºåˆ¶")
            strategies.append("å¯¹é«˜é£é™©ç‰¹å¾å»ºç«‹å®æ—¶é¢„è­¦ç³»ç»Ÿ")
            strategies.append("å®šæœŸå®¡æŸ¥å’Œæ›´æ–°å¼‚å¸¸æ£€æµ‹æ¨¡å‹")
        
        strategies.extend([
            "å»ºç«‹åˆ†çº§å“åº”æœºåˆ¶å¤„ç†ä¸åŒé£é™©ç­‰çº§çš„å¼‚å¸¸",
            "åŠ å¼ºå¯¹å…³é”®ç‰¹å¾çš„æ•°æ®è´¨é‡æ§åˆ¶",
            "å»ºç«‹å¼‚å¸¸å¤„ç†çš„æ ‡å‡†æ“ä½œç¨‹åº",
            "å®šæœŸè¿›è¡Œæ¨¡å‹æ•ˆæœè¯„ä¼°å’Œä¼˜åŒ–"
        ])
        
        return strategies
    
    def _generate_html_report(self, report_data: Dict[str, Any]):
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>å¼‚å¸¸æ£€æµ‹è§£é‡ŠæŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f4f4f4; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .high-risk {{ color: #d32f2f; font-weight: bold; }}
                .medium-risk {{ color: #f57c00; font-weight: bold; }}
                .low-risk {{ color: #388e3c; font-weight: bold; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>åŒ–å·¥å›¾é£æ§ç³»ç»Ÿ - å¼‚å¸¸æ£€æµ‹è§£é‡ŠæŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {report_data['report_metadata']['generation_time']}</p>
                <p>åˆ†æç¯è·¯æ•°: {report_data['report_metadata']['total_loops_analyzed']:,}</p>
            </div>
            
            <div class="section">
                <h2>æ‰§è¡Œæ‘˜è¦</h2>
                <p><strong>æ•´ä½“é£é™©çŠ¶æ€:</strong> <span class="high-risk">{report_data['executive_summary']['overall_risk_status']}</span></p>
                <p><strong>é«˜é£é™©ç¯è·¯æ•°é‡:</strong> {report_data['executive_summary']['high_risk_loops']}</p>
                <p><strong>ä¸»è¦é£é™©å› ç´ :</strong> {', '.join(report_data['executive_summary']['key_risk_factors'])}</p>
            </div>
            
            <div class="section">
                <h2>ç«‹å³è¡ŒåŠ¨å»ºè®®</h2>
                <ul>
        """
        
        for action in report_data['recommendations']['immediate_actions']:
            html_content += f"<li>{action}</li>"
        
        html_content += """
                </ul>
            </div>
            
            <div class="section">
                <h2>ç›‘æ§é‡ç‚¹ç‰¹å¾</h2>
                <ol>
        """
        
        for feature in report_data['recommendations']['monitoring_focus']:
            html_content += f"<li>{feature.replace('_', ' ').title()}</li>"
        
        html_content += """
                </ol>
            </div>
        </body>
        </html>
        """
        
        html_file = self.reports_dir / 'explanation_report.html'
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def run_complete_explanation(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„è§£é‡Šåˆ†ææµç¨‹"""
        logger.info("å¼€å§‹è¿è¡Œå¼‚å¸¸æ£€æµ‹è§£é‡Šåˆ†æ...")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ•°æ®
            if not self.load_data():
                return False
            
            # 2. æ„å»ºè§£é‡Šæ¨¡å‹
            self.build_explanation_models()
            
            # 3. è·³è¿‡å¯è§†åŒ–
            pass
            
            # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = self.generate_comprehensive_report()
            
            total_time = time.time() - start_time
            
            logger.info(f"å¼‚å¸¸æ£€æµ‹è§£é‡Šåˆ†æå®Œæˆï¼")
            logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"åˆ†æäº† {len(self.anomaly_results)} ä¸ªç¯è·¯")
            logger.info(f"ä½¿ç”¨äº† {len(self.feature_names)} ä¸ªç‰¹å¾")
            
            return True
            
        except Exception as e:
            logger.error(f"è§£é‡Šåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("åŒ–å·¥å›¾é£æ§ç³»ç»Ÿ - å¼‚å¸¸æ£€æµ‹è§£é‡Šç³»ç»Ÿ")
    print("ğŸ” ä¸ºå¼‚å¸¸æ£€æµ‹ç»“æœæä¾›è¯¦ç»†çš„å¯è§£é‡Šæ€§åˆ†æ")
    print("=" * 70)
    
    # æ£€æŸ¥ä¾èµ–æ–‡ä»¶
    required_files = [
        'outputs/anomaly_detection/ensemble/final_anomaly_results.csv',
        'outputs/anomaly_detection/features/engineered_features.csv'
    ]
    
    missing_files = []
    for file_path in required_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ æ‰¾ä¸åˆ°å¿…éœ€çš„è¾“å…¥æ–‡ä»¶:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        print("\nğŸ“ è¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹è„šæœ¬:")
        print("   1. python feature_engineering.py")
        print("   2. python anomaly_detection_models.py") 
        print("   3. python anomaly_ensemble_integration.py")
        return
    
    # åˆå§‹åŒ–è§£é‡Šç³»ç»Ÿ
    explainer = AnomalyExplanationSystem()
    
    # è¿è¡Œå®Œæ•´è§£é‡Šæµç¨‹
    success = explainer.run_complete_explanation()
    
    if success:
        print("\n" + "=" * 70)
        print("âœ… å¼‚å¸¸æ£€æµ‹è§£é‡Šåˆ†æå®Œæˆï¼")
        print("ğŸ“Š è¾“å‡ºæ–‡ä»¶:")
        print("   - comprehensive_explanation_report.json (ç»¼åˆåˆ†ææŠ¥å‘Š)")
        print("   - explanation_report.html (HTMLå¯è§†åŒ–æŠ¥å‘Š)")
        print("   - feature_importance_analysis.png (ç‰¹å¾é‡è¦æ€§å›¾)")
        print("   - anomaly_score_distribution.png (å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ)")
        print("   - risk_level_distribution.png (é£é™©ç­‰çº§åˆ†å¸ƒ)")
        print("   - top_anomalies_radar.png (å¼‚å¸¸ç¯è·¯é›·è¾¾å›¾)")
        print("ğŸ“ è¾“å‡ºç›®å½•: outputs/anomaly_detection/")
        print("=" * 70)
    else:
        print("âŒ å¼‚å¸¸æ£€æµ‹è§£é‡Šåˆ†æå¤±è´¥")

if __name__ == "__main__":
    main() 