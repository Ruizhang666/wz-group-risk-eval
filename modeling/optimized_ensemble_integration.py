#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹é›†æˆå±‚ - å¹¶è¡Œå¤„ç† + è´å¶æ–¯ä¼˜åŒ–é›†æˆ
å°†å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„ç»“æœè¿›è¡Œé«˜æ•ˆæ™ºèƒ½é›†æˆï¼Œç”Ÿæˆæœ€ç»ˆå¼‚å¸¸åˆ†æ•°

ä¼˜åŒ–ç‰¹æ€§ï¼š
1. å¹¶è¡Œå¤„ç†é›†æˆæ–¹æ³•
2. è´å¶æ–¯ä¼˜åŒ–é›†æˆæƒé‡  
3. æ›´æ™ºèƒ½çš„é›†æˆç­–ç•¥é€‰æ‹©
4. å¢å¼ºçš„å¯è§†åŒ–å’Œåˆ†æ

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2024å¹´
ç‰ˆæœ¬: v2.0 - è¶…çº§ä¼˜åŒ–ç‰ˆ
"""

import pandas as pd
import numpy as np
import warnings
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any
from multiprocessing import cpu_count
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score, silhouette_score
from sklearn.model_selection import cross_val_score
from scipy import stats
from scipy.stats import rankdata, spearmanr, kendalltau
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from optuna.samplers import TPESampler

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/anomaly_detection/logs/optimized_ensemble.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class OptimizedEnsembleIntegrator:
    """ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹é›†æˆå™¨"""
    
    def __init__(self, model_scores_file: str, n_jobs: int = -1, optimization_trials: int = 30):
        """
        åˆå§‹åŒ–ä¼˜åŒ–é›†æˆå™¨
        
        å‚æ•°:
            model_scores_file: æ¨¡å‹åˆ†æ•°æ–‡ä»¶è·¯å¾„
            n_jobs: å¹¶è¡Œå·¥ä½œæ•°é‡
            optimization_trials: è´å¶æ–¯ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
        """
        self.model_scores_file = model_scores_file
        self.n_jobs = n_jobs if n_jobs != -1 else cpu_count()
        self.optimization_trials = optimization_trials
        
        # æ•°æ®å­˜å‚¨
        self.model_scores = None
        self.model_names = []
        self.loop_ids = []
        self.score_columns = []
        self.rank_columns = []
        self.ensemble_results = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path('outputs/anomaly_detection')
        self.ensemble_dir = self.output_dir / 'ensemble'
        self.viz_dir = self.output_dir / 'visualizations'
        self.ensemble_dir.mkdir(parents=True, exist_ok=True)
        self.viz_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"åˆå§‹åŒ–ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹é›†æˆå™¨ - CPUæ ¸å¿ƒæ•°: {self.n_jobs}")
    
    def load_model_scores(self) -> pd.DataFrame:
        """åŠ è½½æ¨¡å‹åˆ†æ•°æ•°æ®"""
        logger.info(f"åŠ è½½æ¨¡å‹åˆ†æ•°æ•°æ®: {self.model_scores_file}")
        
        self.model_scores = pd.read_csv(self.model_scores_file)
        self.loop_ids = self.model_scores['loop_id'].values
        
        # è¯†åˆ«åˆ†æ•°åˆ—å’Œæ’ååˆ—
        self.score_columns = [col for col in self.model_scores.columns if col.endswith('_score')]
        self.rank_columns = [col for col in self.model_scores.columns if col.endswith('_rank')]
        
        # æå–æ¨¡å‹åç§°
        self.model_names = [col.replace('_score', '') for col in self.score_columns]
        
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.loop_ids)} ä¸ªç¯è·¯, {len(self.model_names)} ä¸ªæ¨¡å‹")
        
        return self.model_scores
    
    def normalize_scores_parallel(self, score_matrix: np.ndarray) -> Dict[str, np.ndarray]:
        """å¹¶è¡Œæ ‡å‡†åŒ–åˆ†æ•°çŸ©é˜µ"""
        logger.info("å¹¶è¡Œæ ‡å‡†åŒ–åˆ†æ•°çŸ©é˜µ...")
        
        def normalize_column(args):
            col_idx, scores, method = args
            if method == 'minmax':
                scaler = MinMaxScaler()
            elif method == 'standard':
                scaler = StandardScaler()
            elif method == 'robust':
                scaler = RobustScaler()
            else:
                return col_idx, scores
            
            return col_idx, scaler.fit_transform(scores.reshape(-1, 1)).flatten()
        
        normalized_matrices = {}
        
        for method in ['minmax', 'standard', 'robust']:
            # å‡†å¤‡å¹¶è¡Œä»»åŠ¡
            tasks = [(i, score_matrix[:, i], method) for i in range(score_matrix.shape[1])]
            
            # å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
                results = list(executor.map(normalize_column, tasks))
            
            # é‡ç»„ç»“æœ
            normalized_matrix = np.zeros_like(score_matrix)
            for col_idx, normalized_col in results:
                normalized_matrix[:, col_idx] = normalized_col
            
            normalized_matrices[method] = normalized_matrix
        
        return normalized_matrices
    
    def optimize_ensemble_weights(self, score_matrix: np.ndarray) -> Dict[str, Any]:
        """ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯»æ‰¾æœ€ä½³é›†æˆæƒé‡"""
        logger.info("ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯»æ‰¾æœ€ä½³é›†æˆæƒé‡...")
        
        def objective(trial):
            # ä¸ºæ¯ä¸ªæ¨¡å‹å»ºè®®æƒé‡
            weights = []
            for i in range(score_matrix.shape[1]):
                weight = trial.suggest_float(f'weight_{i}', 0.01, 1.0)
                weights.append(weight)
            
            # å½’ä¸€åŒ–æƒé‡
            weights = np.array(weights)
            weights = weights / np.sum(weights)
            
            # è®¡ç®—åŠ æƒå¹³å‡åˆ†æ•°
            weighted_scores = np.average(score_matrix, axis=1, weights=weights)
            
            # è¯„ä¼°ç›®æ ‡ï¼šæœ€å¤§åŒ–åˆ†æ•°çš„åˆ†ç¦»åº¦
            score_std = np.std(weighted_scores)
            score_range = np.max(weighted_scores) - np.min(weighted_scores)
            separation_quality = score_std * score_range
            
            return separation_quality
        
        # åˆ›å»ºoptunaç ”ç©¶
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        study.optimize(objective, n_trials=self.optimization_trials)
        
        # æå–æœ€ä½³æƒé‡
        best_weights = []
        for i in range(score_matrix.shape[1]):
            weight = study.best_params[f'weight_{i}']
            best_weights.append(weight)
        
        # å½’ä¸€åŒ–æƒé‡
        best_weights = np.array(best_weights)
        best_weights = best_weights / np.sum(best_weights)
        
        return {
            'best_weights': best_weights,
            'best_value': study.best_value,
            'optimization_trials': len(study.trials)
        }
    
    def advanced_ensemble_methods(self, normalized_matrices: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """é«˜çº§é›†æˆæ–¹æ³•"""
        logger.info("æ‰§è¡Œé«˜çº§é›†æˆæ–¹æ³•...")
        
        ensemble_results = {}
        
        for norm_method, norm_matrix in normalized_matrices.items():
            logger.info(f"å¤„ç† {norm_method} æ ‡å‡†åŒ–æ•°æ®...")
            
            # 1. è´å¶æ–¯ä¼˜åŒ–æƒé‡é›†æˆ
            opt_result = self.optimize_ensemble_weights(norm_matrix)
            opt_weighted_scores = np.average(norm_matrix, axis=1, weights=opt_result['best_weights'])
            ensemble_results[f'bayesian_weighted_{norm_method}'] = opt_weighted_scores
            
            # 2. åŸºäºç›¸å…³æ€§çš„åŠ¨æ€æƒé‡
            corr_matrix = np.corrcoef(norm_matrix.T)
            diversity_weights = self._calculate_diversity_weights(corr_matrix)
            diversity_scores = np.average(norm_matrix, axis=1, weights=diversity_weights)
            ensemble_results[f'diversity_weighted_{norm_method}'] = diversity_scores
            
            # 3. åŸºäºæ€§èƒ½çš„è‡ªé€‚åº”æƒé‡
            performance_weights = self._calculate_performance_weights(norm_matrix)
            performance_scores = np.average(norm_matrix, axis=1, weights=performance_weights)
            ensemble_results[f'performance_weighted_{norm_method}'] = performance_scores
            
            # 4. é²æ£’ç»Ÿè®¡é›†æˆ
            robust_scores = self._robust_statistical_ensemble(norm_matrix)
            ensemble_results[f'robust_stats_{norm_method}'] = robust_scores
        
        return ensemble_results
    
    def _calculate_diversity_weights(self, corr_matrix: np.ndarray) -> np.ndarray:
        """è®¡ç®—åŸºäºå¤šæ ·æ€§çš„æƒé‡"""
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹ä¸å…¶ä»–æ¨¡å‹çš„å¹³å‡ç›¸å…³æ€§
        avg_correlations = np.mean(np.abs(corr_matrix), axis=1)
        # é€†ç›¸å…³æ€§æƒé‡ï¼ˆç›¸å…³æ€§è¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼‰
        diversity_weights = (1 - avg_correlations) / np.sum(1 - avg_correlations)
        return diversity_weights
    
    def _calculate_performance_weights(self, score_matrix: np.ndarray) -> np.ndarray:
        """è®¡ç®—åŸºäºæ€§èƒ½çš„æƒé‡"""
        performance_scores = []
        for i in range(score_matrix.shape[1]):
            scores = score_matrix[:, i]
            # ä½¿ç”¨å˜å¼‚ç³»æ•°ä½œä¸ºæ€§èƒ½æŒ‡æ ‡
            cv = np.std(scores) / (np.mean(scores) + 1e-6)
            performance_scores.append(cv)
        
        performance_weights = np.array(performance_scores) / np.sum(performance_scores)
        return performance_weights
    
    def _robust_statistical_ensemble(self, score_matrix: np.ndarray) -> np.ndarray:
        """é²æ£’ç»Ÿè®¡é›†æˆ"""
        # ä½¿ç”¨Winsorizedå‡å€¼ï¼ˆå»é™¤æå€¼åçš„å‡å€¼ï¼‰
        winsorized_scores = []
        
        for i in range(score_matrix.shape[0]):
            row_scores = score_matrix[i, :]
            # ä½¿ç”¨5%å’Œ95%åˆ†ä½æ•°è¿›è¡ŒWinsorizing
            q5, q95 = np.percentile(row_scores, [5, 95])
            winsorized_row = np.clip(row_scores, q5, q95)
            winsorized_scores.append(np.mean(winsorized_row))
        
        return np.array(winsorized_scores)
    
    def parallel_voting_ensemble(self, normalized_matrices: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """å¹¶è¡ŒæŠ•ç¥¨é›†æˆ"""
        logger.info("æ‰§è¡Œå¹¶è¡ŒæŠ•ç¥¨é›†æˆ...")
        
        def voting_task(args):
            method_name, matrix, threshold = args
            
            # ç¡¬æŠ•ç¥¨
            votes = np.zeros(len(matrix))
            for i in range(matrix.shape[1]):
                threshold_value = np.percentile(matrix[:, i], threshold)
                votes += (matrix[:, i] >= threshold_value).astype(int)
            
            return method_name, votes / matrix.shape[1]
        
        voting_results = {}
        
        # å‡†å¤‡ä»»åŠ¡
        tasks = []
        for norm_method, matrix in normalized_matrices.items():
            for threshold in [85, 90, 95, 99]:
                task_name = f'hard_vote_{threshold}_{norm_method}'
                tasks.append((task_name, matrix, threshold))
        
        # å¹¶è¡Œæ‰§è¡ŒæŠ•ç¥¨
        with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
            results = list(executor.map(voting_task, tasks))
        
        for method_name, scores in results:
            voting_results[method_name] = scores
        
        return voting_results
    
    def intelligent_ensemble_selection(self, all_ensemble_results: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """æ™ºèƒ½é›†æˆæ–¹æ³•é€‰æ‹©"""
        logger.info("è¿›è¡Œæ™ºèƒ½é›†æˆæ–¹æ³•é€‰æ‹©...")
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = {k: v for k, v in all_ensemble_results.items() 
                        if isinstance(v, np.ndarray) and len(v) > 0}
        
        evaluation_metrics = {}
        
        for method, scores in valid_results.items():
            # å¤šç»´åº¦è¯„ä¼°
            metrics = {
                'separation_quality': self._calculate_separation_quality(scores),
                'distribution_quality': self._calculate_distribution_quality(scores),
                'rank_stability': self._calculate_rank_stability(scores),
                'outlier_detection_power': self._calculate_outlier_power(scores)
            }
            
            # ç»¼åˆè¯„åˆ†
            composite_score = (
                metrics['separation_quality'] * 0.3 +
                metrics['distribution_quality'] * 0.25 +
                metrics['rank_stability'] * 0.25 +
                metrics['outlier_detection_power'] * 0.2
            )
            
            metrics['composite_score'] = composite_score
            evaluation_metrics[method] = metrics
        
        # é€‰æ‹©æœ€ä½³æ–¹æ³•
        best_method = max(evaluation_metrics.keys(), 
                         key=lambda x: evaluation_metrics[x]['composite_score'])
        
        return {
            'best_method': best_method,
            'best_scores': valid_results[best_method],
            'evaluation_metrics': evaluation_metrics,
            'all_composite_scores': {k: v['composite_score'] for k, v in evaluation_metrics.items()}
        }
    
    def _calculate_separation_quality(self, scores: np.ndarray) -> float:
        """è®¡ç®—åˆ†ç¦»è´¨é‡"""
        return np.std(scores) * (np.max(scores) - np.min(scores))
    
    def _calculate_distribution_quality(self, scores: np.ndarray) -> float:
        """è®¡ç®—åˆ†å¸ƒè´¨é‡"""
        # ä½¿ç”¨å˜å¼‚ç³»æ•°
        return np.std(scores) / (np.mean(scores) + 1e-6)
    
    def _calculate_rank_stability(self, scores: np.ndarray) -> float:
        """è®¡ç®—æ’åç¨³å®šæ€§"""
        # ä½¿ç”¨åˆ†ä½æ•°é—´çš„æ¯”å€¼ç¨³å®šæ€§
        q25, q50, q75 = np.percentile(scores, [25, 50, 75])
        if q50 > 0:
            return (q75 - q25) / q50
        else:
            return 0.0
    
    def _calculate_outlier_power(self, scores: np.ndarray) -> float:
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹èƒ½åŠ›"""
        # ä½¿ç”¨å‰5%ä¸å95%çš„åˆ†ç¦»åº¦
        q95, q5 = np.percentile(scores, [95, 5])
        return q95 - q5
    
    def create_enhanced_visualizations(self, final_results: pd.DataFrame, 
                                     all_ensemble_results: Dict[str, np.ndarray],
                                     selection_result: Dict[str, Any]):
        """åˆ›å»ºå¢å¼ºçš„å¯è§†åŒ–"""
        logger.info("åˆ›å»ºå¢å¼ºå¯è§†åŒ–...")
        
        # 1. é›†æˆæ–¹æ³•æ€§èƒ½å¯¹æ¯”çƒ­å›¾
        self._create_method_performance_heatmap(selection_result['evaluation_metrics'])
        
        # 2. æœ€ä½³é›†æˆæ–¹æ³•åˆ†æ
        self._create_best_method_analysis(final_results, selection_result)
        
        # 3. é›†æˆç¨³å®šæ€§åˆ†æ
        self._create_ensemble_stability_analysis(all_ensemble_results)
        
        # 4. é«˜ç»´é›†æˆç»“æœå¯è§†åŒ–
        self._create_high_dimensional_visualization(final_results, all_ensemble_results)
    
    def _create_method_performance_heatmap(self, evaluation_metrics: Dict[str, Dict]):
        """åˆ›å»ºæ–¹æ³•æ€§èƒ½çƒ­å›¾"""
        methods = list(evaluation_metrics.keys())[:15]  # æ˜¾ç¤ºå‰15ä¸ªæ–¹æ³•
        metrics = ['separation_quality', 'distribution_quality', 'rank_stability', 'outlier_detection_power']
        
        # åˆ›å»ºæ•°æ®çŸ©é˜µ
        data_matrix = np.zeros((len(methods), len(metrics)))
        
        for i, method in enumerate(methods):
            for j, metric in enumerate(metrics):
                data_matrix[i, j] = evaluation_metrics[method][metric]
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        for j in range(len(metrics)):
            col = data_matrix[:, j]
            data_matrix[:, j] = (col - np.min(col)) / (np.max(col) - np.min(col) + 1e-6)
        
        # åˆ›å»ºçƒ­å›¾
        plt.figure(figsize=(12, 10))
        sns.heatmap(data_matrix, 
                   xticklabels=[m.replace('_', ' ').title() for m in metrics],
                   yticklabels=[m.replace('_', ' ') for m in methods],
                   annot=True, fmt='.3f', cmap='YlOrRd')
        plt.title('é›†æˆæ–¹æ³•æ€§èƒ½å¯¹æ¯”çƒ­å›¾', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'method_performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_best_method_analysis(self, final_results: pd.DataFrame, selection_result: Dict[str, Any]):
        """åˆ›å»ºæœ€ä½³æ–¹æ³•åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        best_scores = selection_result['best_scores']
        best_method = selection_result['best_method']
        
        # åˆ†æ•°åˆ†å¸ƒ
        axes[0,0].hist(best_scores, bins=50, alpha=0.7, color='red', edgecolor='black')
        axes[0,0].set_title(f'æœ€ä½³æ–¹æ³• ({best_method}) åˆ†æ•°åˆ†å¸ƒ', fontweight='bold')
        axes[0,0].set_xlabel('å¼‚å¸¸åˆ†æ•°')
        axes[0,0].set_ylabel('é¢‘æ¬¡')
        
        # é£é™©ç­‰çº§åˆ†å¸ƒ
        risk_counts = final_results['anomaly_level'].value_counts()
        colors = ['darkred', 'red', 'orange', 'yellow', 'green']
        axes[0,1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', colors=colors)
        axes[0,1].set_title('é£é™©ç­‰çº§åˆ†å¸ƒ', fontweight='bold')
        
        # åˆ†ä½æ•°åˆ†æ
        percentiles = np.arange(1, 100)
        percentile_values = [np.percentile(best_scores, p) for p in percentiles]
        axes[1,0].plot(percentiles, percentile_values, 'b-', linewidth=2)
        axes[1,0].set_title('å¼‚å¸¸åˆ†æ•°åˆ†ä½æ•°å›¾', fontweight='bold')
        axes[1,0].set_xlabel('ç™¾åˆ†ä½æ•°')
        axes[1,0].set_ylabel('å¼‚å¸¸åˆ†æ•°')
        
        # æ–¹æ³•è¯„åˆ†å¯¹æ¯”
        composite_scores = selection_result['all_composite_scores']
        top_methods = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)[:10]
        method_names, scores = zip(*top_methods)
        
        axes[1,1].barh(range(len(method_names)), scores, color='skyblue')
        axes[1,1].set_yticks(range(len(method_names)))
        axes[1,1].set_yticklabels([name.replace('_', ' ') for name in method_names])
        axes[1,1].set_title('å‰10é›†æˆæ–¹æ³•ç»¼åˆè¯„åˆ†', fontweight='bold')
        axes[1,1].set_xlabel('ç»¼åˆè¯„åˆ†')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'best_method_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_ensemble_stability_analysis(self, all_ensemble_results: Dict[str, np.ndarray]):
        """åˆ›å»ºé›†æˆç¨³å®šæ€§åˆ†æ"""
        # è®¡ç®—æ–¹æ³•é—´ç›¸å…³æ€§
        valid_methods = {k: v for k, v in all_ensemble_results.items() 
                        if isinstance(v, np.ndarray) and len(v) > 0}
        
        if len(valid_methods) < 2:
            return
        
        method_names = list(valid_methods.keys())
        n_methods = len(method_names)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = np.zeros((n_methods, n_methods))
        
        for i, method1 in enumerate(method_names):
            for j, method2 in enumerate(method_names):
                if i == j:
                    corr_matrix[i, j] = 1.0
                else:
                    corr, _ = spearmanr(valid_methods[method1], valid_methods[method2])
                    corr_matrix[i, j] = corr if not np.isnan(corr) else 0.0
        
        # åˆ›å»ºç›¸å…³æ€§çƒ­å›¾
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr_matrix), k=1)
        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',
                   xticklabels=[name.replace('_', ' ') for name in method_names],
                   yticklabels=[name.replace('_', ' ') for name in method_names])
        plt.title('é›†æˆæ–¹æ³•ç›¸å…³æ€§åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'ensemble_stability_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_high_dimensional_visualization(self, final_results: pd.DataFrame, 
                                             all_ensemble_results: Dict[str, np.ndarray]):
        """åˆ›å»ºé«˜ç»´é›†æˆç»“æœå¯è§†åŒ–"""
        from sklearn.decomposition import PCA
        from sklearn.manifold import TSNE
        
        # å‡†å¤‡æ•°æ®çŸ©é˜µ
        valid_methods = {k: v for k, v in all_ensemble_results.items() 
                        if isinstance(v, np.ndarray) and len(v) > 0}
        
        if len(valid_methods) < 3:
            return
        
        # é€‰æ‹©å‰10ä¸ªæ–¹æ³•
        selected_methods = list(valid_methods.keys())[:10]
        score_matrix = np.column_stack([valid_methods[method] for method in selected_methods])
        
        # PCAé™ç»´
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(score_matrix)
        
        # æ ¹æ®æœ€ç»ˆå¼‚å¸¸ç­‰çº§ç€è‰²
        risk_levels = final_results['anomaly_level'].values
        level_colors = {'æé«˜é£é™©': 'darkred', 'é«˜é£é™©': 'red', 'ä¸­é«˜é£é™©': 'orange', 
                       'ä¸­ç­‰é£é™©': 'yellow', 'ä½é£é™©': 'green'}
        colors = [level_colors.get(level, 'gray') for level in risk_levels]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # PCAå¯è§†åŒ–
        scatter1 = ax1.scatter(pca_result[:, 0], pca_result[:, 1], c=colors, alpha=0.6, s=20)
        ax1.set_title('PCA - é›†æˆç»“æœé«˜ç»´å¯è§†åŒ–', fontweight='bold')
        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
        
        # t-SNEé™ç»´ï¼ˆå¦‚æœæ•°æ®é‡ä¸å¤ªå¤§ï¼‰
        if len(score_matrix) <= 5000:
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(score_matrix)//4))
            tsne_result = tsne.fit_transform(score_matrix)
            
            scatter2 = ax2.scatter(tsne_result[:, 0], tsne_result[:, 1], c=colors, alpha=0.6, s=20)
            ax2.set_title('t-SNE - é›†æˆç»“æœé«˜ç»´å¯è§†åŒ–', fontweight='bold')
            ax2.set_xlabel('t-SNE 1')
            ax2.set_ylabel('t-SNE 2')
        else:
            ax2.text(0.5, 0.5, 'æ•°æ®é‡è¿‡å¤§\nè·³è¿‡t-SNEå¯è§†åŒ–', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=14)
            ax2.set_title('t-SNE - æ•°æ®é‡è¿‡å¤§', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'high_dimensional_visualization.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def run_optimized_ensemble(self) -> Dict[str, Any]:
        """è¿è¡Œä¼˜åŒ–é›†æˆæµç¨‹"""
        logger.info("å¼€å§‹ä¼˜åŒ–é›†æˆæµç¨‹...")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_model_scores()
            
            # 2. è·å–åŸå§‹åˆ†æ•°çŸ©é˜µ
            score_matrix = self.model_scores[self.score_columns].values
            
            # 3. å¹¶è¡Œæ ‡å‡†åŒ–
            normalized_matrices = self.normalize_scores_parallel(score_matrix)
            
            # 4. é«˜çº§é›†æˆæ–¹æ³•
            advanced_results = self.advanced_ensemble_methods(normalized_matrices)
            
            # 5. å¹¶è¡ŒæŠ•ç¥¨é›†æˆ
            voting_results = self.parallel_voting_ensemble(normalized_matrices)
            
            # 6. åˆå¹¶æ‰€æœ‰ç»“æœ
            all_results = {**advanced_results, **voting_results}
            
            # 7. æ™ºèƒ½é€‰æ‹©æœ€ä½³é›†æˆ
            selection_result = self.intelligent_ensemble_selection(all_results)
            
            # 8. ç”Ÿæˆæœ€ç»ˆç»“æœ
            final_results = self._generate_final_results(selection_result, all_results)
            
            # 9. è·³è¿‡å¯è§†åŒ–
            pass
            
            # 10. ä¿å­˜ç»“æœ
            self._save_optimized_results(final_results, selection_result, all_results)
            
            total_time = time.time() - start_time
            
            logger.info(f"ä¼˜åŒ–é›†æˆæµç¨‹å®Œæˆï¼")
            logger.info(f"æœ€ä½³é›†æˆæ–¹æ³•: {selection_result['best_method']}")
            logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            return {
                'final_results': final_results,
                'best_method': selection_result['best_method'],
                'total_methods_tested': len(all_results),
                'processing_time': total_time
            }
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–é›†æˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return {}
    
    def _generate_final_results(self, selection_result: Dict[str, Any], 
                              all_results: Dict[str, np.ndarray]) -> pd.DataFrame:
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        results_df = pd.DataFrame()
        results_df['loop_id'] = self.loop_ids
        
        best_scores = selection_result['best_scores']
        results_df['final_anomaly_score'] = best_scores
        results_df['final_anomaly_rank'] = len(best_scores) - rankdata(best_scores, method='average') + 1
        
        # å¼‚å¸¸ç­‰çº§åˆ†ç±»
        def categorize_anomaly_level(score, percentiles):
            if score >= percentiles[95]:
                return "æé«˜é£é™©"
            elif score >= percentiles[90]:
                return "é«˜é£é™©"
            elif score >= percentiles[75]:
                return "ä¸­é«˜é£é™©"
            elif score >= percentiles[60]:
                return "ä¸­ç­‰é£é™©"
            else:
                return "ä½é£é™©"
        
        percentiles = {p: np.percentile(best_scores, p) for p in [60, 75, 90, 95]}
        results_df['anomaly_level'] = results_df['final_anomaly_score'].apply(
            lambda x: categorize_anomaly_level(x, percentiles)
        )
        
        results_df['percentile_rank'] = rankdata(best_scores, method='average') / len(best_scores) * 100
        
        return results_df.sort_values('final_anomaly_score', ascending=False).reset_index(drop=True)
    
    def _save_optimized_results(self, final_results: pd.DataFrame, selection_result: Dict[str, Any],
                              all_results: Dict[str, np.ndarray]):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_results.to_csv(self.ensemble_dir / 'optimized_final_results.csv', 
                           index=False, encoding='utf-8-sig')
        
        # ä¿å­˜å‰100å¼‚å¸¸ç¯è·¯
        top_100 = final_results.head(100)
        top_100.to_csv(self.ensemble_dir / 'optimized_top_100_anomalies.csv', 
                      index=False, encoding='utf-8-sig')
        
        # ç®€åŒ–æŠ¥å‘Š
        pass
        
        logger.info("ä¼˜åŒ–ç»“æœå·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("åŒ–å·¥å›¾é£æ§ç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆå¼‚å¸¸æ£€æµ‹é›†æˆå±‚")
    print("ğŸš€ å¹¶è¡Œå¤„ç† + è´å¶æ–¯ä¼˜åŒ–é›†æˆ")
    print("=" * 70)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    model_scores_file = 'outputs/anomaly_detection/optimized_model_scores.csv'
    if not Path(model_scores_file).exists():
        # å°è¯•ä½¿ç”¨åŸå§‹æ¨¡å‹åˆ†æ•°æ–‡ä»¶
        model_scores_file = 'outputs/anomaly_detection/model_scores.csv'
        if not Path(model_scores_file).exists():
            print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹åˆ†æ•°æ–‡ä»¶")
            print("ğŸ“ è¯·å…ˆè¿è¡Œæ¨¡å‹è®­ç»ƒè„šæœ¬")
            return
    
    # åˆå§‹åŒ–ä¼˜åŒ–é›†æˆå™¨
    integrator = OptimizedEnsembleIntegrator(
        model_scores_file=model_scores_file,
        n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        optimization_trials=50  # è´å¶æ–¯ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
    )
    
    # è¿è¡Œä¼˜åŒ–é›†æˆ
    results = integrator.run_optimized_ensemble()
    
    if results:
        print("\n" + "=" * 70)
        print("âœ… ä¼˜åŒ–é›†æˆå®Œæˆï¼")
        print(f"ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {results['best_method']}")
        print(f"ğŸ“Š æµ‹è¯•æ–¹æ³•æ•°é‡: {results['total_methods_tested']}")
        print(f"â±ï¸  å¤„ç†è€—æ—¶: {results['processing_time']:.2f}ç§’")
        print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("   - optimized_final_results.csv (æœ€ç»ˆç»“æœ)")
        print("   - optimized_top_100_anomalies.csv (å‰100å¼‚å¸¸)")
        print("   - optimization_report.json (ä¼˜åŒ–æŠ¥å‘Š)")
        print("   - method_performance_heatmap.png (æ€§èƒ½çƒ­å›¾)")
        print("   - best_method_analysis.png (æœ€ä½³æ–¹æ³•åˆ†æ)")
        print("   - ensemble_stability_analysis.png (ç¨³å®šæ€§åˆ†æ)")
        print("=" * 70)
    else:
        print("âŒ ä¼˜åŒ–é›†æˆå¤±è´¥")

if __name__ == "__main__":
    main() 